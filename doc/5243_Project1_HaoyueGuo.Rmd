---
title: "Does happiness moment vary by developed country and develpoving country ?"

output:
  html_document:
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### This report explore the happy moment in dovelped country and develping country conditional on three different group, whcih are teenagers from age 10 to 19, parents under age 55 and old people over age 60. 

##### I use USA as represent of developed country and India as represent of developing country.

##### HappyDB is a corpus of 100,000 crowd-sourced happy moments via Amazon's Mechanical Turk. You can read more about it on https://arxiv.org/abs/1801.07746, I employed the subset Cleaned_hm.cvs and demographic.csv in here.







###### Part 1 Text_Processing
I cleaned the data and conbined it with demographic data by directly using the code in Text_Processing and HappyDB_RShiny that are provide in project.

### Step 0 - Load all the required libraries

From the packages' descriptions:

+ `tm` is a framework for text mining applications within R;
+ `tidyverse` is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures;
+ `tidytext` allows text mining using 'dplyr', 'ggplot2', and other tidy tools;
+ `DT` provides an R interface to the JavaScript library DataTables.

```{r load libraries, warning=FALSE, message=FALSE}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
```

### Step 1 - Load the data to be cleaned and processed

```{r read data, warning=FALSE, message=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
```

### Step 2 - Preliminary cleaning of text

We clean the text by converting all the letters to the lower case, and removing punctuation, numbers, empty words and extra white space.

```{r text processing in tm}
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)
```

### Step 3 - Stemming words and converting tm object to tidy object

Stemming reduces a word to its word *stem*. We stem the words here and then convert the "tm" object to a "tidy" object for much faster processing.

```{r stemming}
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)
```

### Step 4 - Creating tidy format of the dictionary to be used for completing stems

We also need a dictionary to look up the words corresponding to the stems.

```{r tidy dictionary}
dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)
```

### Step 5 - Removing stopwords that don't hold any significant information for our data set

We remove stopwords provided by the "tidytext" package and also add custom stopwords in context of our data.

```{r stopwords}
data("stop_words")

word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")

stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))
```

### Step 6 - Combining stems and dictionary into the same tibble

Here we combine the stems and the dictionary into the same "tidy" object.

```{r tidy stems with dictionary}
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) %>%
  anti_join(stop_words, by = c("dictionary" = "word"))
```

### Step 7 - Stem completion

Lastly, we complete the stems by picking the corresponding word with the highest frequency.

```{r stem completion, warning=FALSE, message=FALSE}
completed <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)
```

### Step 8 - Pasting stem completed individual words into their respective happy moments

We want our processed words to resemble the structure of the original happy moments. So we paste the words together to form happy moments.

```{r reverse unnest}
completed <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()
```

### Step 9 - Keeping a track of the happy moments with their own ID

```{r cleaned hm_data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  mutate(id = row_number()) %>%
  inner_join(completed)

```

### Exporting the processed text data into a CSV file

```{r export data}
write_csv(hm_data, "/Users/dengpan/Desktop/GHY/processed_moments.csv")
```

The final processed data is ready to be used for any kind of analysis.





###### Part 1 continued, Part of codes of HappyDB_RSHiny


```{r load libraries, warning=FALSE, message=FALSE}

library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny) 
```

### Step 1 - Load the processed text data along with demographic information on contributors

We use the processed data for our analysis and combine it with the demographic information available.

```{r load data, warning=FALSE, message=FALSE}
hm_data <- read_csv("/Users/dengpan/Desktop/GHY/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```

### Combine both the data sets and keep the required columns for analysis

We select a subset of the data that satisfies specific row conditions.

```{r combining data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(country %in% c("USA", "IND")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))


```


###### Part 2 Data analysis and visualization starts in here, after the data processing


##### Overall Level
Make a words bag/data for "USA" and "IND"
```{r}
hm_data_USA<-hm_data[hm_data$country=="USA",]
hm_data_IND<-hm_data[hm_data$country=="IND",]
```

```{r, message=FALSE,echo=FALSE}
bag_of_words_USA <-  hm_data_USA %>%
  unnest_tokens(word, text)

word_count_USA <- bag_of_words_USA %>%
  count(word, sort = TRUE)

bag_of_words_IND <-  hm_data_IND %>%
  unnest_tokens(word, text)

word_count_IND <- bag_of_words_IND %>%
  count(word, sort = TRUE)
```

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_USA$word,word_count_USA$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Oranges"))

```


```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_USA[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for USA")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_IND$word,word_count_IND$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```

```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_IND[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for India")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 
In over level, there are not much differences between USA and India, so we break the population down to do further analyze. The first population we focus on is the teenagers. 

##### Focus on teenager from age 10 t0 age 19
```{r}
hm_data_USA_teenager <- hm_data[hm_data$country == "USA" & hm_data$age >= 10 & hm_data$age <= 19, ]

hm_data_IND_teenager <- hm_data[hm_data$country == "USA" & hm_data$age >= 10 & hm_data$age <= 19, ]

```

```{r, message=FALSE,echo=FALSE}
bag_of_words_USA_teenager <-  hm_data_USA_teenager %>%
  unnest_tokens(word, text)

word_count_USA_teenager <- bag_of_words_USA_teenager %>%
  count(word, sort = TRUE)

bag_of_words_IND_teenager<-  hm_data_IND_teenager %>%
  unnest_tokens(word, text)

word_count_IND_teenager <- bag_of_words_IND_teenager %>%
  count(word, sort = TRUE)
```

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_USA_teenager$word,word_count_USA_teenager$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Oranges"))

```


```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_USA_teenager[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for teenagers in USA")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_IND_teenager$word,word_count_IND_teenager$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```

```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_IND_teenager[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for teenagers in India")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 
I found that teenagers in both countries care more about friends, games, events. This seems satisfies the common sense, teenagers like entertainment events. The second population I interest is the parents under age 55, the reason of this setting is I think parent over 55 care less about their kid since their kids already grew up at that age. 

##### Focus on parents under age 55
```{r}
hm_data_USA_parent <- hm_data[hm_data$country == "USA" & hm_data$parenthood=="y" & hm_data$age <= 55, ]

hm_data_IND_parent <- hm_data[hm_data$country == "IND" & hm_data$parenthood=="y" & hm_data$age <= 55, ]

```

```{r, message=FALSE,echo=FALSE}
bag_of_words_USA_parent <-  hm_data_USA_parent %>%
  unnest_tokens(word, text)

word_count_USA_parent <- bag_of_words_USA_parent %>%
  count(word, sort = TRUE)

bag_of_words_IND_parent <-  hm_data_IND_parent %>%
  unnest_tokens(word, text)

word_count_IND_parent <- bag_of_words_IND_parent %>%
  count(word, sort = TRUE)
```

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_USA_parent$word,word_count_USA_parent$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Oranges"))

```


```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_USA_parent[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for parents in USA")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_IND_parent$word,word_count_IND_parent$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```

```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_IND_parent[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for parents in India")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 
The results tell us a really interesting story in here. Parents in United state do care more about their children, however parents in India care more about friends than kids.The reason behind this could be Indian cultrues or its society makes their people pay more attention on surviving and networking rather than children. The third populations we care about is the old people over 60.

##### Focus on old people whose age over 60
```{r}
hm_data_USA_old <- hm_data[hm_data$country == "USA" & hm_data$age >= 60, ]

hm_data_IND_old <- hm_data[hm_data$country == "IND" & hm_data$age >= 60, ]

```

```{r, message=FALSE,echo=FALSE}
bag_of_words_USA_old <-  hm_data_USA_old %>%
  unnest_tokens(word, text)

word_count_USA_old <- bag_of_words_USA_old %>%
  count(word, sort = TRUE)

bag_of_words_IND_old <-  hm_data_IND_old %>%
  unnest_tokens(word, text)

word_count_IND_old <- bag_of_words_IND_old %>%
  count(word, sort = TRUE)
```

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_USA_old$word,word_count_USA_old$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Oranges"))

```


```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_USA_old[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for old in USA")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 

```{r,message=FALSE,warning=FALSE,echo=FALSE}
wordcloud(word_count_IND_old$word,word_count_IND_old$n ,
          scale=c(3,0.1),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```

```{r,warning=FALSE, message=FALSE,echo=FALSE}
 word_count_IND_old[1:15,] %>%
      mutate(word = reorder(word, n)) %>%
      ggplot(aes(word, n)) +
      geom_col() +
      labs(title = "               Word Frequency in Happy Moments for old in India")+
      xlab(NULL) +
      ylab("Word Frequency")+
      coord_flip()
``` 
As the age grows over 60, the result shows that India people care more about their kid compared to USA people, the situation flips. This might because Indian people time and effort to accumulate wealth that can support their life compared to USA people. Once people accumulated certain amount wealth they would pay more attention to their kids, in words kids can influence their happ moment more. 
